{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwwBXvIbUXA_"
   },
   "source": [
    "# Contrastive Language-Image Pretraining with SogCLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVyLXcCiUkeF"
   },
   "source": [
    "### **Introduction**\n",
    "\n",
    "In this tutorial, you will learn how to conduct contrastive language-image pretraining by optimizing the [Global Contrastive Loss](https://arxiv.org/abs/2202.12387) (GCL) on a subset of the [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) dataset. Also, you will learn how to evaluate the model on retrieval task using the [MSCOCO](https://cocodataset.org/#home) dataset and zero-shot classification task using the [ImageNet](https://www.image-net.org/challenges/LSVRC/index.php) dataset. The code is based on [iSogCLR's](https://github.com/zhqiu/contrastive-learning-iSogCLR) codebase, which includes the implementation of CLIP, SogCLR and iSogCLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsO954DCVdgn"
   },
   "source": [
    "### Preparation\n",
    "\n",
    "First, we:\n",
    "\n",
    "1. Download the source code and data\n",
    "2. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3wc5FDn51l6"
   },
   "outputs": [],
   "source": [
    "!git clone -b project https://github.com/xywei00/csce689_iSogCLR.git iSogCLR\n",
    "\n",
    "!export PYTHONPATH=\"$PYTHONPATH:./iSogCLR/bimodal_exps\"\n",
    "!export HUGGINGFACE_HUB_CACHE='./checkpoints/huggingface'\n",
    "!mkdir checkpoints\n",
    "\n",
    "!gdown 142xxRoMaHxX3BIfCw_1b_G_dgu-02Yq3    # clip_train.tar.gz\n",
    "!gdown 142zQjlOw0Xw4tKzXMrQjYE6NtGRTeasT    # cc3m_subset_100k.tar.gz\n",
    "!gdown 142tMsnclHTTPpnTXHSeNgTUlBk4She6o    # ms_coco_val.tar.gz\n",
    "!gdown 1NXhfhwFy-nhdABACkodgYqm9pomDKE39    # val.tar\n",
    "\n",
    "!mkdir datasets\n",
    "!mkdir -p datasets/imagenet\n",
    "!tar xf clip_train.tar.gz\n",
    "!tar xf cc3m_subset_100k.tar.gz -C datasets\n",
    "!tar xf mscoco_val.tar.gz -C datasets\n",
    "!tar xf val.tar -C datasets/imagenet\n",
    "\n",
    "!pip install -r ./iSogCLR/requirements_colab.txt    # there may be pip warnings/ errors, should be fine to ignore them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11x28L3vV2od"
   },
   "source": [
    "### Training\n",
    "\n",
    "The following command runs the training script to train a ResNet50 (pretrained on ImageNet) and a DistilBERT (pretrained on BookCorpus and English Wikipedia) on the cc3m dataset using the SogCLR loss for 30 epochs with temperature 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4BjOwHiWP2u"
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python ./iSogCLR/bimodal_exps/clip.py \\\n",
    "    --data_path ./datasets \\\n",
    "    --ann_path ./clip_train \\\n",
    "    --train_file cc3m_train_subset.json \\\n",
    "    --train_image_root cc3m_subset_100k \\\n",
    "    --output_dir output/sogclr_cc3m_g0.8_e30 \\\n",
    "    --init_model \\\n",
    "    --use_amp \\\n",
    "    --ita_type sogclr \\\n",
    "    --tau_init 0.01 \\\n",
    "    --sogclr_gamma 0.8 \\\n",
    "    --eta_init 0.03 --sched cosine \\\n",
    "    --no-distributed \\\n",
    "    --epochs 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmCh9QFuWx-t"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "The following command runs the evaluation script to evaluate the retrieval performance of the trained model on the MSCOCO validation dataset and the zero-shot classification performance on the ImageNet validation dataset. The evaluation command is obtained by appending `--evaluate --checkpoint /path/to/your/checkpoint --zs_dataset imagenet --zs_datafolder /path/to/imagenet/val` to the training command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdSq-cQwoork"
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python ./iSogCLR/bimodal_exps/clip.py \\\n",
    "    --data_path ./datasets \\\n",
    "    --ann_path ./clip_train \\\n",
    "    --train_file cc3m_train_subset.json \\\n",
    "    --train_image_root cc3m_subset_100k \\\n",
    "    --output_dir output/isogclr_cc3m_g0.8_e30 \\\n",
    "    --init_model \\\n",
    "    --use_amp \\\n",
    "    --ita_type sogclr \\\n",
    "    --tau_init 0.01 \\\n",
    "    --sogclr_gamma 0.8 \\\n",
    "    --eta_init 0.03 --sched cosine \\\n",
    "    --no-distributed \\\n",
    "    --epochs 30 \\\n",
    "    --evaluate --checkpoint './output/sogclr_cc3m_g0.8_e30/checkpoint_30.pth' \\\n",
    "    --zs_dataset imagenet --zs_datafolder ./datasets/imagenet/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4tw47loXXPK"
   },
   "source": [
    "### Benchmarks\n",
    "\n",
    "The following results are recall at 1 results on the provided MSCOCO and ImageNet datasets. The first row of results are from the model trained using the CLIP loss, and the second row of results are from the model trained using the SogCLR loss. All results are based on a batch size of 128 for 30-epoch pretraining. IR@1 denotes the recall at 1 of image retrieval on MSCOCO, TR@1 denotes the recall at 1 of text retrieval on MSCOCO, and ACC@1 denotes the top 1 accuracy on ImageNet. Average denotes the average of the three metrics.\n",
    "\n",
    "| Method | MSCOCO TR@1 | MSCOCO IR@1 | ImageNet ACC@1 | Average |\n",
    "|:----------:|:--------:|:--------:|:--------:|:--------:|\n",
    "| CLIP | 12.0 | 9.32 | 21.35 | 14.22 |\n",
    "| SogCLR |  14.38  |  10.73  | 24.54 | 16.55 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define new loss functions\n",
    "def vicreg_loss(preds, targets, sim_coeff=25.0, std_coeff=25.0, cov_coeff=1.0):\n",
    "    \"\"\"VICReg loss implementation\"\"\"\n",
    "    # Similarity term\n",
    "    sim_loss = F.mse_loss(preds, targets)\n",
    "    # Variance term\n",
    "    std_pred = torch.std(preds, dim=0) + 1e-4\n",
    "    std_targ = torch.std(targets, dim=0) + 1e-4\n",
    "    std_loss = torch.mean(F.relu(1 - std_pred)) + torch.mean(F.relu(1 - std_targ))\n",
    "    # Covariance term\n",
    "    cov_pred = preds.T @ preds / (preds.shape[0] - 1)\n",
    "    cov_targ = targets.T @ targets / (targets.shape[0] - 1)\n",
    "    cov_loss = torch.sum(cov_pred ** 2) + torch.sum(cov_targ ** 2)\n",
    "    return sim_coeff * sim_loss + std_coeff * std_loss + cov_coeff * cov_loss\n",
    "\n",
    "def supcon_loss(features, labels, temperature=0.07):\n",
    "    \"\"\"Supervised Contrastive Loss\"\"\"\n",
    "    device = features.device\n",
    "    labels = labels.unsqueeze(-1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(device)\n",
    "    logits = torch.div(torch.matmul(features, features.T), temperature)\n",
    "    exp_logits = torch.exp(logits) * (1 - torch.eye(features.shape[0]).to(device))\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "    return -mean_log_prob_pos.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa667525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize optimizers\n",
    "def get_optimizer(model, optimizer_name, lr=2e-4, weight_decay=1e-2):\n",
    "    if optimizer_name == \"adamw\":\n",
    "        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop with different loss functions\n",
    "for loss_function_name in [\"global_contrastive\", \"vicreg\", \"supcon\"]:\n",
    "    for optimizer_name in [\"adamw\", \"sgd\"]:\n",
    "        print(f\"Training with {loss_function_name} and {optimizer_name}\")\n",
    "        optimizer = get_optimizer(model, optimizer_name)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in train_loader:\n",
    "                images, texts, labels = batch\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                image_features = model.visual_encoder(images)\n",
    "                text_features = model.text_encoder(texts)\n",
    "\n",
    "                # Compute loss\n",
    "                if loss_function_name == \"global_contrastive\":\n",
    "                    loss = global_contrastive_loss(image_features, text_features)\n",
    "                elif loss_function_name == \"vicreg\":\n",
    "                    loss = vicreg_loss(image_features, text_features)\n",
    "                elif loss_function_name == \"supcon\":\n",
    "                    loss = supcon_loss(image_features, labels)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported loss function: {loss_function_name}\")\n",
    "\n",
    "                # Backward and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Log metrics\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0053b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation for retrieval and zero-shot classification\n",
    "def evaluate(model, val_loader, metric=\"recall@1\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        results = []\n",
    "        for batch in val_loader:\n",
    "            images, texts = batch\n",
    "            image_features = model.visual_encoder(images)\n",
    "            text_features = model.text_encoder(texts)\n",
    "\n",
    "            if metric == \"recall@1\":\n",
    "                # Calculate recall@1\n",
    "                pass  # Add your Recall@1 calculation\n",
    "            elif metric == \"zeroshot_top1\":\n",
    "                # Calculate zero-shot top-1 accuracy\n",
    "                pass  # Add your zero-shot accuracy calculation\n",
    "\n",
    "            results.append(metric_result)\n",
    "    return np.mean(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b66a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log results for each experiment\n",
    "experiment_results = {}\n",
    "for loss_function_name, optimizer_name in experiments:\n",
    "    result_key = f\"{loss_function_name}_{optimizer_name}\"\n",
    "    experiment_results[result_key] = evaluate(model, val_loader)\n",
    "\n",
    "# Print and save results\n",
    "print(experiment_results)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
